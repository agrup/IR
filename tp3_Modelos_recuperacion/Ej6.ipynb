{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Recuperaciòn de Informaciòn (y evaluaciòn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la colecci´on provista por el equipo docente1\n",
    ", cuya estructura es la siguiente:\n",
    "    \n",
    "vocabulary.txt → [id termino, idf, t´ermino]\n",
    "\n",
    "documentVectors.txt → [id doc, lista(id terminos)]\n",
    "\n",
    "queries.txt → [id query, lista(id terminos)]\n",
    "\n",
    "relevants.txt → [id query, listarelevantes (id doc)]\n",
    "\n",
    "informationNeeds.txt → [id in, texto libre]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Calcule los conjuntos de respuestas usando el modelo booleano y el modelo vectorial (asuma en todos\n",
    "los casos T F = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "import math\n",
    "from modulos.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idf(corpus_count,doc_freq):\n",
    "    if(doc_freq !=0):\n",
    "        return math.log(corpus_count/doc_freq,2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(dirname):\n",
    "        files = listdir(dirname)\n",
    "        vocabulary={}\n",
    "        document_vector={}\n",
    "        id_voc=0\n",
    "        vocavulary_result=[]\n",
    "        docs_count=len(files)\n",
    "        for file in files:\n",
    "            lines = open(dirname+'/'+file,'r',errors = 'ignore').readlines()\n",
    "            docu_voc=[]\n",
    "            tokens=tokenizar(lines)\n",
    "            #docs_count+=1\n",
    "            for token in tokens:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token]=(id_voc,1)\n",
    "                    docu_voc.append(id_voc)\n",
    "                    id_voc =id_voc+1\n",
    "                    #print(token,docu_voc)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    id,doc_freq = vocabulary[token]\n",
    "                    \n",
    "                    #print(id,docu_voc)\n",
    "                    if id not in docu_voc:\n",
    "                       \n",
    "                        doc_freq = doc_freq+1\n",
    "                        docu_voc.append(id)\n",
    "                        \n",
    "                    vocabulary[token]=(id,doc_freq)\n",
    "                    \n",
    "            document_vector[file]=docu_voc\n",
    "        #print(document_vector)\n",
    "        #print(vocabulary)\n",
    "        for termn in vocabulary.items():\n",
    "            #print(termn[1][1],docs_count)\n",
    "            vocavulary_result.append([termn[1][0],calc_idf(docs_count,termn[1][1]),termn[0]])\n",
    "        #print (vocavulary_result)\n",
    "        return (vocavulary_result,document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.5849625007211562, 'bye'], [1, 1.5849625007211563, 'hose'], [2, 1.5849625007211563, 'house'], [3, 0.0, 'dog'], [4, 1.5849625007211563, 'http'], [5, 1.5849625007211563, 'url'], [6, 1.5849625007211563, 'com'], [7, 1.5849625007211563, 'ar'], [8, 1.5849625007211563, 'agustin'], [9, 1.5849625007211563, 'rodriguez'], [10, 0.5849625007211562, 'agu'], [11, 1.5849625007211563, 'gmail'], [12, 1.5849625007211563, 'www'], [13, 1.5849625007211563, 'https'], [14, 1.5849625007211563, 'pepe'], [15, 1.5849625007211563, 'run'], [16, 0.5849625007211562, 'hello'], [17, 1.5849625007211563, 'sleep'], [18, 0.5849625007211562, 'program'], [19, 0.5849625007211562, 'programer'], [20, 1.5849625007211563, 'lic'], [21, 1.5849625007211563, 'programs'], [22, 1.5849625007211563, 'programing']]\n",
      "{'ir.txt': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'file1.txt': [15, 16, 3, 17, 18, 19, 20, 10], 'archivo1.txt': [16, 0, 3, 18, 21, 19, 22]}\n"
     ]
    }
   ],
   "source": [
    "vocabulary,documents=  indexer('/home/agu/Unlu/IR/tp2_analisis_texto/data')\n",
    "print(vocabulary)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.5849625007211562, 'bye'], [1, 1.5849625007211563, 'hose'], [2, 1.5849625007211563, 'house'], [3, 0.0, 'dog'], [4, 1.5849625007211563, 'http'], [5, 1.5849625007211563, 'url'], [6, 1.5849625007211563, 'com'], [7, 1.5849625007211563, 'ar'], [8, 1.5849625007211563, 'agustin'], [9, 1.5849625007211563, 'rodriguez'], [10, 0.5849625007211562, 'agu'], [11, 1.5849625007211563, 'gmail'], [12, 1.5849625007211563, 'www'], [13, 1.5849625007211563, 'https'], [14, 1.5849625007211563, 'pepe'], [15, 1.5849625007211563, 'run'], [16, 0.5849625007211562, 'hello'], [17, 1.5849625007211563, 'sleep'], [18, 0.5849625007211562, 'program'], [19, 0.5849625007211562, 'programer'], [20, 1.5849625007211563, 'lic'], [21, 1.5849625007211563, 'programs'], [22, 1.5849625007211563, 'programing']]\n",
      "{'ir.txt': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'file1.txt': [15, 16, 3, 17, 18, 19, 20, 10], 'archivo1.txt': [16, 0, 3, 18, 21, 19, 22]}\n"
     ]
    }
   ],
   "source": [
    "#voc = (list(get_vocabulary(\"ejemploRibeiro/vocabulary.txt\", True)))\n",
    "#doc = (get_document_vector(\"ejemploRibeiro/documentVector.txt\", True))\n",
    "voc = vocabulary\n",
    "vocabulary_list=list(map(itemgetter(2), voc))\n",
    "#doc = documents\n",
    "#doc_bool= to_boolean(doc,voc)\n",
    "print (voc)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(query):\n",
    "    \n",
    "    query_result=[]\n",
    "    querys = query.split()\n",
    "    \n",
    "    for term  in querys:\n",
    "        if term in vocabulary_list:\n",
    "            query_result.append(vocabulary_list.index(term))  \n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "query =get_query(\"hello dog agustin\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file1.txt', 'archivo1.txt', 'ir.txt']\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "query_documents=[]\n",
    "for term in query:\n",
    "\n",
    "\n",
    "    for document, values in documents.items():\n",
    "\n",
    "        if term in values:\n",
    "            if document not in query_documents:   \n",
    "                query_documents.append(document)\n",
    "\n",
    "print(query_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal(list_term,vocabulary):\n",
    "    query_normal=0\n",
    "    for term in list_term:\n",
    "        val=(vocabulary[term][1])\n",
    "        query_normal+=float(val)**2\n",
    "    query_normal=math.sqrt(query_normal)\n",
    "    #print(query_normal,\"fnc\")\n",
    "    return(query_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 3, 8]\n",
      "Boolean Result\n",
      "file1.txt\n",
      "archivo1.txt\n",
      "ir.txt\n",
      "------------------------\n",
      "Vectorial Result\n",
      "1.6894636000642955\n",
      "('ir.txt', 0.9381453975456103)\n",
      "('file1.txt', 0.34624155305796134)\n",
      "('archivo1.txt', 0.34624155305796134)\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "print(query)\n",
    "#print(get_normal(query,voc))\n",
    "query_normal=get_normal(query,voc)\n",
    "\n",
    "result_q=[]\n",
    "\n",
    "terms_isin_documents={}\n",
    "\n",
    "for term in query:\n",
    "    if voc[term][1] > 0:\n",
    "        query_document=[]\n",
    "        #query_document=[]\n",
    "        #for document in doc_bool:\n",
    "        for document, values in documents.items():\n",
    "            #docu, value = doc\n",
    "            if term in values:\n",
    "            #if (value[term-1]):\n",
    "                #a =[ids for ids, _ in query_document]\n",
    "                #print (a)\n",
    "                #print(docu[0])\n",
    "                if (document not in [ids for ids, _ in result_q]):\n",
    "                    query_document.append((document,voc[term][1]))\n",
    "\n",
    "            if(term in terms_isin_documents.keys()):\n",
    "                 #print(terms_isin_documents.keys())\n",
    "                #print(query_document)\n",
    "                terms_isin_documents[str(term)].extend(query_document)     \n",
    "            else:\n",
    "                if len(query_document)>0:\n",
    "                    terms_isin_documents[str(term)]=query_document\n",
    "        result_q.extend(query_document)\n",
    "#print(\"query\",query_document) \n",
    "result.append((\"query\",result_q))\n",
    "\n",
    "print(\"Boolean Result\")\n",
    "for doc,_ in result_q:\n",
    "    print(doc)\n",
    "#print(result_q)\n",
    "query_vector=[]\n",
    "for key,value in terms_isin_documents.items():\n",
    "    query_vector.append(float(voc[int(key)][1]))\n",
    "\n",
    "\n",
    "vector_result=[]\n",
    "for name,idf in result_q:\n",
    "    #print(result_q)\n",
    "    vect_sim =[]\n",
    "    sim=0\n",
    "    #value son los documentos que contienen los terminos\n",
    "    #print(terms_isin_documents.items())\n",
    "    for key,value in terms_isin_documents.items():\n",
    "        if (name in [val[0] for val in value]):\n",
    "            vect_sim.append(float(idf))\n",
    "        else:\n",
    "            vect_sim.append(0)\n",
    "    vector_result.append((name,vect_sim))\n",
    "print(\"------------------------\")\n",
    "print(\"Vectorial Result\")\n",
    "#print(vector_result)\n",
    "list_response=[]\n",
    "print(query_normal)\n",
    "for doc,vec in vector_result:\n",
    "    #r=np.array(r)\n",
    "    q=0\n",
    "\n",
    "    for idf in vec:\n",
    "        q+=idf**2\n",
    "    q = math.sqrt(q)\n",
    "    q = (np.dot(vec,query_vector)) /(query_normal*q)\n",
    "    list_response.append((doc,q))\n",
    "\n",
    "\n",
    "(list_response.sort(key=itemgetter(1),reverse=True))\n",
    "\n",
    "for doc in list_response:\n",
    "    print(doc)\n",
    "print(\"-----------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
